{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, f1_score, classification_report, balanced_accuracy_score, roc_auc_score, roc_curve, auc, recall_score, precision_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output = pd.read_csv(\"data/output.csv\")\n",
    "df_politics = pd.read_csv(\"data/cleaned_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the politeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_politeness = pd.read_csv(\"data/politeness_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_politics = df_politics.merge(df_politeness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_politics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representation check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = (df_politics['political_leaning'].value_counts() / len(df_politics)) * 100\n",
    "print(baseline)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying standard linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_politics.value_counts('political_leaning').plot(kind='bar', fontsize=10, xlabel = \"political_leaning\", ylabel=\"Number of posts\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column called political_leaning_id which maps political_leaning to a numerical value with -1 being left, 0 being center, and 1 being right\n",
    "# this is done because the model cannot take in string values\n",
    "df_politics['political_leaning_id'] = df_politics['political_leaning'].map({'left': 0, 'center': 1, 'right': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = df_politics['political_leaning_id'].corr(df_politics['amount_of_cursewords'])\n",
    "df_new = df_politics[['political_leaning_id', 'amount_of_cursewords']].copy()\n",
    "correlation_matrix = df_new.corr()\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_politics.drop(['Unnamed: 0', 'username', 'post', 'political_leaning', 'cleaned_post', 'political_leaning_id'], axis=1)\n",
    "Y = df_politics['political_leaning_id']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# add constant\n",
    "X_train = sm.add_constant(X_train)\n",
    "\n",
    "# performing the regression and fitting the model\n",
    "results = sm.OLS(y_train, X_train).fit()\n",
    "\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significant = ['amount_of_cursewords', 'Hedges', 'Impersonal.Pronoun', 'Swearing', 'Negation', 'Filler.Pause', 'Informal.Title', 'Goodbye', 'For.Me', 'Reasoning', 'Reassurance', 'Ask.Agency', 'Give.Agency', 'First.Person.Plural', 'First.Person.Single', 'Second.Person', 'Third.Person', 'Positive.Emotion', 'Negative.Emotion', 'Questions', 'Apology', 'Truth.Intensifier', 'Conjunction.Start']\n",
    "super_significant = ['amount_of_cursewords', 'Hedges', 'Impersonal.Pronoun', 'Swearing', 'Negation', 'Filler.Pause', 'Informal.Title', 'Reasoning', 'First.Person.Plural', 'First.Person.Single', 'Second.Person', 'Questions', 'Apology', 'Truth.Intensifier']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking a closer look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate where std 1-3 of the data is below\n",
    "threesigma = df_politics['amount_of_cursewords'].quantile(0.997)\n",
    "twosigma = df_politics['amount_of_cursewords'].quantile(0.95)\n",
    "onesigma = df_politics['amount_of_cursewords'].quantile(0.68)\n",
    "print(threesigma, twosigma, onesigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_politics[df_politics['amount_of_cursewords'] >= 52]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_politics_cleaned = df_politics[df_politics['amount_of_cursewords'] <= 52]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Wordcloud en mooie plaatje:\n",
    "\n",
    "# df_right = df_politics_cleaned[df_politics_cleaned['political_leaning'] == 'right']\n",
    "# df_left = df_politics_cleaned[df_politics_cleaned['political_leaning'] == 'left']\n",
    "# df_centered = df_politics_cleaned[df_politics_cleaned['political_leaning'] == 'center']\n",
    "# right_text = \" \".join(word for word in df_right.post)\n",
    "# print(\"There are {} words in the combination of all review.\".format(len(right_text)))\n",
    "# left_text = \" \".join(word for word in df_left.post)\n",
    "# print(\"There are {} words in the combination of all review.\".format(len(left_text)))\n",
    "# centered_text = \" \".join(word for word in df_centered.post)\n",
    "# print(\"There are {} words in the combination of all review.\".format(len(centered_text)))\n",
    "# # Create and generate a word cloud image:\n",
    "# wordcloud = WordCloud().generate(right_text)\n",
    "\n",
    "# # Display the generated image:\n",
    "# plt.imshow(wordcloud, interpolation='bilinear')\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordcloud = WordCloud().generate(left_text)\n",
    "\n",
    "# # Display the generated image:\n",
    "# plt.imshow(wordcloud, interpolation='bilinear')\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_politics_cleaned.value_counts('amount_of_cursewords').plot(kind='bar', fontsize=10, xlabel = \"Number of curse words\", ylabel=\"Number of posts\", title=\"Number of curse words per post\", xticks=np.arange(0, 61, 10))\n",
    "\n",
    "#plot the 1, 2, and 3 sigma lines\n",
    "plt.axvline(x=onesigma, color='r', linestyle='-')\n",
    "plt.axvline(x=twosigma, color='r', linestyle='-')\n",
    "plt.axvline(x=threesigma, color='r', linestyle='-')\n",
    "\n",
    "# annotate the 1, 2, and 3 sigma lines\n",
    "plt.text(onesigma, 3500, '68%', rotation=90)\n",
    "plt.text(twosigma, 3500, '95%', rotation=90)\n",
    "plt.text(threesigma, 3500, '99.7%', rotation=90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmas = dict()\n",
    "for word in super_significant:\n",
    "    threesigma = df_politics[word].quantile(0.997)\n",
    "    extreme_count = len(df_politics[df_politics[word] >= threesigma])\n",
    "    print(word, threesigma, extreme_count)\n",
    "    plot = df_politics_cleaned.value_counts(word).plot(kind='bar', fontsize=10, xlabel = word, ylabel=\"Number of posts\", title=word, xticks=np.arange(0, 61, 10));\n",
    "    plt.axvline(x=threesigma, color='r', linestyle='-')\n",
    "    plt.show()\n",
    "    sigmas[word] = threesigma\n",
    "\n",
    "df_politics_super_cleaned = df_politics.copy()\n",
    "for word in sigmas:\n",
    "    df_politics_super_cleaned = df_politics_super_cleaned[df_politics_super_cleaned[word] <= sigmas[word]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a violin plot with the political leaning on the x-axis and the amount of curse words on the y-axis\n",
    "sns.violinplot(x=\"political_leaning\", y=\"amount_of_cursewords\", data=df_politics_super_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the correlation with 3Ïƒ of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = df_politics_super_cleaned['political_leaning_id'].corr(df_politics_super_cleaned['amount_of_cursewords'])\n",
    "df_new = df_politics_super_cleaned[['political_leaning_id', 'amount_of_cursewords']].copy()\n",
    "correlation_matrix = df_new.corr()\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_politics_super_cleaned[super_significant]\n",
    "Y = df_politics_super_cleaned['political_leaning_id']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# add constant\n",
    "X_train = sm.add_constant(X_train)\n",
    "\n",
    "# performing the regression and fitting the model\n",
    "results = sm.OLS(y_train, X_train).fit()\n",
    "\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features (X) and target (Y)\n",
    "X = df_politics_super_cleaned[super_significant]\n",
    "Y = df_politics_super_cleaned['political_leaning_id']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(\"F1 score:\", f1)\n",
    "\n",
    "# --- Hyperparameter Tuning ---\n",
    "param_dist = {'n_estimators': randint(50,500),\n",
    "              'max_depth': randint(1,20),\n",
    "              'max_features': randint(1,14)}\n",
    "\n",
    "# for grid search\n",
    "# forest_params = [{\n",
    "#     'n_estimators': [50, 100, 200, 300, 400, 500],\n",
    "#     'max_depth': [list(range(5, 15))], \n",
    "#     'max_features': list(range(6,14))\n",
    "#     }]\n",
    "\n",
    "\n",
    "# Create a random forest classifier\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Use random search to find the best hyperparameters\n",
    "rand_search = RandomizedSearchCV(rf, \n",
    "                                 param_distributions = param_dist, \n",
    "                                 n_iter=10, \n",
    "                                 cv=5)\n",
    "\n",
    "# Fit the random search object to the data\n",
    "rand_search.fit(X_train, y_train)\n",
    "\n",
    "# Create a variable for the best model\n",
    "best_rf = rand_search.best_estimator_\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print('Best hyperparameters:',  rand_search.best_params_)\n",
    "\n",
    "# --- end of hyperparameter tuning ---\n",
    "\n",
    "# Generate predictions with the best model\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "print('\\n------------------ Confusion Matrix -----------------\\n')\n",
    "ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, y_pred)).plot();\n",
    "plt.show()\n",
    "print('\\n-------------------- Key Metrics --------------------')\n",
    "print('\\nAccuracy: {:.2f}'.format(accuracy_score(y_test, y_pred)))\n",
    "print('Balanced Accuracy: {:.2f}\\n'.format(balanced_accuracy_score(y_test, y_pred)))\n",
    "\n",
    "print('Micro Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='micro')))\n",
    "print('Micro Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='micro')))\n",
    "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_test, y_pred, average='micro')))\n",
    "\n",
    "print('Macro Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='macro')))\n",
    "print('Macro Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='macro')))\n",
    "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_test, y_pred, average='macro')))\n",
    "\n",
    "print('Weighted Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='weighted')))\n",
    "print('Weighted Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='weighted')))\n",
    "print('Weighted F1-score: {:.2f}\\n'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "\n",
    "print('\\n--------------- Classification Report ---------------\\n')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features (X) and target (Y)\n",
    "X = df_politics_super_cleaned[super_significant]\n",
    "Y = df_politics_super_cleaned['political_leaning_id']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "logreg = LogisticRegression(random_state=42, max_iter=1000)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print('\\n------------------ Confusion Matrix -----------------\\n')\n",
    "ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, y_pred)).plot();\n",
    "plt.show()\n",
    "print('\\n-------------------- Key Metrics --------------------')\n",
    "print('\\nAccuracy: {:.2f}'.format(accuracy_score(y_test, y_pred)))\n",
    "print('Balanced Accuracy: {:.2f}\\n'.format(balanced_accuracy_score(y_test, y_pred)))\n",
    "\n",
    "print('Micro Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='micro')))\n",
    "print('Micro Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='micro')))\n",
    "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_test, y_pred, average='micro')))\n",
    "\n",
    "print('Macro Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='macro')))\n",
    "print('Macro Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='macro')))\n",
    "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_test, y_pred, average='macro')))\n",
    "\n",
    "print('Weighted Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='weighted')))\n",
    "print('Weighted Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='weighted')))\n",
    "print('Weighted F1-score: {:.2f}\\n'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "\n",
    "print('\\n--------------- Classification Report ---------------\\n')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features (X) and target (Y)\n",
    "X = df_politics_super_cleaned[super_significant]\n",
    "Y = df_politics_super_cleaned['political_leaning_id']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "      % (X_test.shape[0], (y_test != y_pred).sum()))\n",
    "\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "print('\\n------------------ Confusion Matrix -----------------\\n')\n",
    "ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, y_pred)).plot();\n",
    "plt.show()\n",
    "print('\\n-------------------- Key Metrics --------------------')\n",
    "print('\\nAccuracy: {:.2f}'.format(accuracy_score(y_test, y_pred)))\n",
    "print('Balanced Accuracy: {:.2f}\\n'.format(balanced_accuracy_score(y_test, y_pred)))\n",
    "\n",
    "print('Micro Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='micro')))\n",
    "print('Micro Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='micro')))\n",
    "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_test, y_pred, average='micro')))\n",
    "\n",
    "print('Macro Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='macro')))\n",
    "print('Macro Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='macro')))\n",
    "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_test, y_pred, average='macro')))\n",
    "\n",
    "print('Weighted Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='weighted')))\n",
    "print('Weighted Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='weighted')))\n",
    "print('Weighted F1-score: {:.2f}\\n'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "\n",
    "print('\\n--------------- Classification Report ---------------\\n')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bare Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features (X) and target (Y)\n",
    "X = df_politics_super_cleaned[super_significant]\n",
    "Y = df_politics_super_cleaned['political_leaning_id']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "\n",
    "clf = clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('\\n------------------ Confusion Matrix -----------------\\n')\n",
    "ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, y_pred)).plot();\n",
    "plt.show()\n",
    "print('\\n-------------------- Key Metrics --------------------')\n",
    "print('\\nAccuracy: {:.2f}'.format(accuracy_score(y_test, y_pred)))\n",
    "print('Balanced Accuracy: {:.2f}\\n'.format(balanced_accuracy_score(y_test, y_pred)))\n",
    "\n",
    "print('Micro Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='micro')))\n",
    "print('Micro Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='micro')))\n",
    "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_test, y_pred, average='micro')))\n",
    "\n",
    "print('Macro Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='macro')))\n",
    "print('Macro Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='macro')))\n",
    "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_test, y_pred, average='macro')))\n",
    "\n",
    "print('Weighted Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='weighted')))\n",
    "print('Weighted Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='weighted')))\n",
    "print('Weighted F1-score: {:.2f}\\n'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "\n",
    "print('\\n--------------- Classification Report ---------------\\n')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on https://www.kaggle.com/code/emmanuelfwerr/xgboost-multi-class-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features (X) and target (Y)\n",
    "X = df_politics[super_significant]\n",
    "Y = df_politics['political_leaning_id']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "## ---------- XGBoost model v1 ----------\n",
    "## base run of model with default hyperparameters\n",
    "\n",
    "xgb_clf = xgb.XGBClassifier(objective='multi:softmax', \n",
    "                            num_class=3, \n",
    "                            early_stopping_rounds=10, \n",
    "                            eval_metric=['merror', 'auc', 'mlogloss'], \n",
    "                            seed=42)\n",
    "xgb_clf.fit(X_train, \n",
    "            y_train,\n",
    "            verbose=0, # set to 1 to see xgb training round intermediate results\n",
    "            eval_set=[(X_train, y_train), (X_test, y_test)])\n",
    "\n",
    "# preparing evaluation metric plots\n",
    "results = xgb_clf.evals_result()\n",
    "epochs = len(results['validation_0']['mlogloss'])\n",
    "x_axis = range(0, epochs)\n",
    "\n",
    "# xgboost 'mlogloss' plot\n",
    "fig, ax = plt.subplots(figsize=(9,5))\n",
    "ax.plot(x_axis, results['validation_0']['mlogloss'], label='Train')\n",
    "ax.plot(x_axis, results['validation_1']['mlogloss'], label='Test')\n",
    "ax.legend()\n",
    "plt.ylabel('mlogloss')\n",
    "plt.title('GridSearchCV XGBoost mlogloss')\n",
    "plt.show()\n",
    "\n",
    "# xgboost 'merror' plot\n",
    "fig, ax = plt.subplots(figsize=(9,5))\n",
    "ax.plot(x_axis, results['validation_0']['merror'], label='Train')\n",
    "ax.plot(x_axis, results['validation_1']['merror'], label='Test')\n",
    "ax.legend()\n",
    "plt.ylabel('merror')\n",
    "plt.title('GridSearchCV XGBoost merror')\n",
    "plt.show()\n",
    "\n",
    "## ---------- Model Classification Report ----------\n",
    "## get predictions and create model quality report\n",
    "\n",
    "y_pred = xgb_clf.predict(X_test)\n",
    "\n",
    "print('\\n------------------ Confusion Matrix -----------------\\n')\n",
    "ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, y_pred)).plot();\n",
    "plt.show()\n",
    "print('\\n-------------------- Key Metrics --------------------')\n",
    "print('\\nAccuracy: {:.2f}'.format(accuracy_score(y_test, y_pred)))\n",
    "print('Balanced Accuracy: {:.2f}\\n'.format(balanced_accuracy_score(y_test, y_pred)))\n",
    "\n",
    "print('Micro Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='micro')))\n",
    "print('Micro Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='micro')))\n",
    "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_test, y_pred, average='micro')))\n",
    "\n",
    "print('Macro Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='macro')))\n",
    "print('Macro Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='macro')))\n",
    "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_test, y_pred, average='macro')))\n",
    "\n",
    "print('Weighted Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='weighted')))\n",
    "print('Weighted Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='weighted')))\n",
    "print('Weighted F1-score: {:.2f}\\n'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "\n",
    "print('\\n--------------- Classification Report ---------------\\n')\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('---------------------- XGBoost ----------------------') # unnecessary fancy styling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
